{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_IbFciNTpNV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "galKGCKrhWx6"
      },
      "source": [
        "# Utils folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEOkQvg8UEUJ"
      },
      "source": [
        "# Vocab.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "953KUA-gUCVU"
      },
      "source": [
        "import torch\n",
        "\n",
        "class Vocab():\n",
        "    def __init__(self,embed,word2id):\n",
        "        self.embed = embed      #given embeddings\n",
        "        self.word2id = word2id  #in form word:id\n",
        "        self.id2word = {v:k for k,v in word2id.items()} #id:word\n",
        "        assert len(self.word2id) == len(self.id2word) #this condition always should be true\n",
        "        self.PAD_IDX = 0\n",
        "        self.UNK_IDX = 1\n",
        "        self.PAD_TOKEN = 'PAD_TOKEN'\n",
        "        self.UNK_TOKEN = 'UNK_TOKEN'\n",
        "\n",
        "    def __len__(self): #returns length of the words\n",
        "        return len(word2id)\n",
        "\n",
        "    def i2w(self,idx): #returns words when the index is given\n",
        "        return self.id2word[idx]\n",
        "    def w2i(self,w):\n",
        "        if w in self.word2id: #when a word is given  return its index\n",
        "            return self.word2id[w]\n",
        "        else:\n",
        "            return self.UNK_IDX #if word doesn't exist return 1(UNK_IDX)\n",
        "\n",
        "    def make_features(self,batch,sent_trunc=50,doc_trunc=100,split_token='\\n'): #max sent and word lengths\n",
        "        sents_list,targets,doc_lens = [],[],[]\n",
        "        # trunc document\n",
        "        for doc,label in zip(batch['doc'],batch['labels']): #zip clubs corresponding inputs (doc,label) here\n",
        "            sents = doc.split(split_token) #sentencification of article\n",
        "            labels = label.split(split_token)\n",
        "            labels = [int(l) for l in labels]\n",
        "            max_sent_num = min(doc_trunc,len(sents)) #getting max size of sentences\n",
        "            sents = sents[:max_sent_num] #truncating sentences if greater than fixed size\n",
        "            labels = labels[:max_sent_num] #truncating labels if greater then fixed size\n",
        "            sents_list += sents #big list of all sentencified articles\n",
        "            targets += labels #big list of all labels of sentencified articles\n",
        "            doc_lens.append(len(sents)) #collecting all doc lengths\n",
        "        # trunc or pad sent\n",
        "        max_sent_len = 0\n",
        "        batch_sents = []\n",
        "        for sent in sents_list:\n",
        "            words = sent.split()\n",
        "            if len(words) > sent_trunc:\n",
        "                words = words[:sent_trunc] #truncating the words in a sentences to fixed length\n",
        "            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
        "            batch_sents.append(words) #it have all the tokenized sentences list\n",
        "\n",
        "        features = []\n",
        "        for sent in batch_sents:\n",
        "            feature = [self.w2i(w) for w in sent] + [self.PAD_IDX for _ in range(max_sent_len-len(sent))] #features have indices of all the words of s sentence\n",
        "            features.append(feature) #it have features of all the tokenized sentences(list)\n",
        "\n",
        "        features = torch.LongTensor(features)    #changing features(numbers) to tensors\n",
        "        targets = torch.LongTensor(targets) #changing labels to tensors\n",
        "        summaries = batch['summaries'] #batch is given as input during the function call and it have summaries in it\n",
        "\n",
        "        return features,targets,summaries,doc_lens\n",
        "\n",
        "    def make_predict_features(self, batch, sent_trunc=150, doc_trunc=100, split_token='. '):\n",
        "        sents_list, doc_lens = [],[]\n",
        "        for doc in batch: #from the passed batch taking each doc or article\n",
        "            sents = doc.split(split_token) #sentencification is done using '.'(fullstop)\n",
        "            max_sent_num = min(doc_trunc,len(sents))  #getting maximum len among doc_truc and len(sentence)\n",
        "            sents = sents[:max_sent_num]  #trucating article (having limited number of sentences in the article)\n",
        "            sents_list += sents\n",
        "            doc_lens.append(len(sents)) #appending the length of each sentence\n",
        "        # trunc or pad sent\n",
        "        max_sent_len = 0\n",
        "        batch_sents = []\n",
        "        for sent in sents_list: #similarly taking each sentence from the sentence list and further it is truncated based on the number of words to be in each sentence\n",
        "            words = sent.split()\n",
        "            if len(words) > sent_trunc:\n",
        "                words = words[:sent_trunc]\n",
        "            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
        "            batch_sents.append(words) #it is then appended to another biglist\n",
        "\n",
        "        features = []\n",
        "        for sent in batch_sents:  #features are taken for the predicted output\n",
        "            feature = [self.w2i(w) for w in sent] + [self.PAD_IDX for _ in range(max_sent_len-len(sent))]\n",
        "            features.append(feature)\n",
        "\n",
        "        features = torch.LongTensor(features)\n",
        "\n",
        "        return features, doc_lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3w6P94ThVg1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3j5Sqf7ElOu"
      },
      "source": [
        "# Dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITbliXckEloX"
      },
      "source": [
        "import csv\n",
        "import torch\n",
        "import torch.utils.data as data #Dataset stores the samples and their corresponding labels can be existing or user data\n",
        "from torch.autograd import Variable\n",
        "#from .Vocab import Vocab      #importing the above vocab class\n",
        "import numpy as np\n",
        "\n",
        "class Dataset(data.Dataset): #this is the dataset in torch.utils.data\n",
        "    def __init__(self, examples):\n",
        "        super(Dataset,self).__init__()      #\n",
        "        # data: {'sents':xxxx,'labels':'xxxx', 'summaries':[1,0]}\n",
        "        self.examples = examples\n",
        "        self.training = False\n",
        "    def train(self):\n",
        "        self.training = True #enabling training\n",
        "        return self\n",
        "    def test(self):\n",
        "        self.training = False #disabling training while testing\n",
        "        return self\n",
        "    def shuffle(self,words):\n",
        "        np.random.shuffle(words)    #order of the sub-arrays changes but their content remains the same\n",
        "        return ' '.join(words)      #here all the words will be in a string with a space as seperator\n",
        "    def dropout(self,words,p=0.3):\n",
        "        l = len(words)\n",
        "        drop_index = np.random.choice(l,int(l*p)) #we can get the random samples of one dimensional array and return the random samples of numpy array, here l-1D array & int(l*p) is return size\n",
        "        keep_words = [words[i] for i in range(l) if i not in drop_index] #removing some samples from the whole set of words\n",
        "        return ' '.join(keep_words) #returning this keep_words as a string with seperator space\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx] #with given index getting the word at that index\n",
        "        return ex\n",
        "        #words = ex['sents'].split()\n",
        "        #guess = np.random.random()\n",
        "\n",
        "        #if self.training:\n",
        "        #    if guess > 0.5:\n",
        "        #        sents = self.dropout(words,p=0.3)\n",
        "        #    else:\n",
        "        #        sents = self.shuffle(words)\n",
        "        #else:\n",
        "        #    sents = ex['sents']\n",
        "        #return {'id':ex['id'],'sents':sents,'labels':ex['labels']}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples) #returning the length of examples that is passed during the declaration of the class object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXnmllBVUUbU"
      },
      "source": [
        "#from .Dataset import Dataset\n",
        "#from .Vocab import Vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk10sAJIhbTe"
      },
      "source": [
        "# Models folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSqXKnuhTv-t"
      },
      "source": [
        "# Model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGNfqz-uUlTt"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable #used to create variable that provides backward method in backpropogation Eg.stores loss values and cals gradient in backprop.\n",
        "class BasicModule(torch.nn.Module): #torch.nn.Module : It is a base class used to develop all neural network models. Inheritance\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super(BasicModule,self).__init__() #in super() the first parameter is the subclass, and the second parameter is an object that is an instance of that subclass.\n",
        "        self.args = args\n",
        "        self.model_name = str(type(self))   #\n",
        "\n",
        "    def pad_doc(self,words_out,doc_lens): # words_out is tensor in pytorch\n",
        "        pad_dim = words_out.size(1)       #tensors size of 1D at 2nd position i.e., at index 1\n",
        "        max_doc_len = max(doc_lens)       #\n",
        "        sent_input = []\n",
        "        start = 0\n",
        "        for doc_len in doc_lens:\n",
        "            stop = start + doc_len\n",
        "            valid = words_out[start:stop]     #considering article wise                                  # (doc_len,2*H)\n",
        "            start = stop\n",
        "            if doc_len == max_doc_len:\n",
        "                sent_input.append(valid.unsqueeze(0)) #unsqueeze changes 2D to 3D tensor eg.[[1,2],[3,4]] to [[[1,2],[3,4]]]\n",
        "            else:\n",
        "                pad = Variable(torch.zeros(max_doc_len-doc_len,pad_dim)) #torch.zeros(2,3) gives two rows and three columns filled with zeroes\n",
        "                if self.args.device is not None:\n",
        "                    pad = pad.cuda()        #\n",
        "                sent_input.append(torch.cat([valid,pad]).unsqueeze(0))    #concatenation of eg.t1=[[1,2],[3,4]], t2=[[5,6],[7,8]] cat--> [[1,2],[3,4],[5,6],[7,8]] # (1,max_len,2*H)\n",
        "        sent_input = torch.cat(sent_input,dim=0)  #concatinating all the (unsqeezed) tensors                              # (B,max_len,2*H)\n",
        "        return sent_input\n",
        "\n",
        "    def save(self):\n",
        "        checkpoint = {'model':self.state_dict(), 'args': self.args}         #state_dict maps to models_parameters that are in torch.nn.module , state_dict is a dictionary that has learnable parameter which can be modified as it is dict\n",
        "        best_path = '%s%s_seed_%d.pt' % (self.args.save_dir,self.model_name,self.args.seed)\n",
        "        torch.save(checkpoint,best_path) #saves an object to a disk file\n",
        "\n",
        "        return best_path\n",
        "\n",
        "    def load(self, best_path):\n",
        "        if self.args.device is not None:\n",
        "            data = torch.load(best_path)['model'] #load(file like obj) loding model(key) from the file like object\n",
        "        else:\n",
        "            data = torch.load(best_path, map_location=lambda storage, loc: storage)['model'] #if device is none then gettng it from storage\n",
        "        self.load_state_dict(data)    #here data is dictionary (checkpoint in above funtion) is taken out using load_state_dict\n",
        "        if self.args.device is not None:\n",
        "            return self.cuda() #CUDA is a parallel computing platform and programming model that enables dramatic increases in computing performance\n",
        "        else:\n",
        "            return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44nyAC5mU4dz"
      },
      "source": [
        "#from .BasicModule import BasicModule"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGtJEy23hhg0"
      },
      "source": [
        "# proprocessing and main program files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H3QmxVMT0eS"
      },
      "source": [
        "# Preprocessor.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bP0lHC9T06l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "3a7762e8-f4b7-460e-c351-d8be59ecf201"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from glob import glob\n",
        "from time import time\n",
        "from multiprocessing import Pool,cpu_count\n",
        "from itertools import chain\n",
        "\n",
        "def build_vocab(args):\n",
        "    print('start building vocab')\n",
        "\n",
        "    PAD_IDX = 0\n",
        "    UNK_IDX = 1\n",
        "    PAD_TOKEN = 'PAD_TOKEN'\n",
        "    UNK_TOKEN = 'UNK_TOKEN'\n",
        "\n",
        "    f = open(args.embed) #it is given in args\n",
        "    embed_dim = int(next(f).split()[1])\n",
        "\n",
        "    word2id = OrderedDict() #OrderedDict preserves the order in which the keys are inserted\n",
        "\n",
        "    word2id[PAD_TOKEN] = PAD_IDX\n",
        "    word2id[UNK_TOKEN] = UNK_IDX\n",
        "\n",
        "    embed_list = []\n",
        "    # fill PAD and UNK vector\n",
        "    embed_list.append([0 for _ in range(embed_dim)]) #[[0,0,.....]]\n",
        "    embed_list.append([0 for _ in range(embed_dim)]) #[[0,0,.....],[0,0,.....]]\n",
        "\n",
        "    # build Vocab\n",
        "    for line in f:\n",
        "        tokens = line.split() #This is not 1D\n",
        "        word = tokens[:-1*embed_dim][0] #taking reverse of the 0th positional elements\n",
        "        vector = [float(num) for num in tokens[-1*embed_dim:]]  #here vector is floating number format of words\n",
        "        embed_list.append(vector)   #[[0,0,.....],[0,0,.....],[vector],[vector],....]\n",
        "        word2id[word] = len(word2id) #giving words as keys and their length as value to the dictionary word2id\n",
        "    f.close()\n",
        "    embed = np.array(embed_list,dtype=np.float32) #creates an array\n",
        "    np.savez_compressed(file=args.vocab, embedding=embed) #Save several arrays into a single file in compressed .npz format\n",
        "    with open(args.word2id,'w') as f:\n",
        "        json.dump(word2id,f)    #converts the Python objects into appropriate json objects and writing them into the file word2id.json file\n",
        "\n",
        "def worker(files):\n",
        "    examples = []\n",
        "    for f in files:\n",
        "        parts = open(f,encoding='latin-1').read().split('\\n\\n')\n",
        "        try:\n",
        "            entities = { line.strip().split(':')[0]:line.strip().split(':')[1].lower() for line in parts[-1].split('\\n')}\n",
        "        except:\n",
        "            continue\n",
        "        sents,labels,summaries = [],[],[]\n",
        "        # content\n",
        "        for line in parts[1].strip().split('\\n'):\n",
        "            content, label = line.split('\\t\\t\\t')\n",
        "            tokens = content.strip().split()\n",
        "            for i,token in enumerate(tokens):\n",
        "                if token in entities:\n",
        "                    tokens[i] = entities[token]\n",
        "            label = '1' if label == '1' else '0'\n",
        "            sents.append(' '.join(tokens))\n",
        "            labels.append(label)\n",
        "        # summary\n",
        "        for line in parts[2].strip().split('\\n'):\n",
        "            tokens = line.strip().split()\n",
        "            for i, token in enumerate(tokens):\n",
        "                if token in entities:\n",
        "                    tokens[i] = entities[token]\n",
        "            line = ' '.join(tokens).replace('*','')\n",
        "            summaries.append(line)\n",
        "        ex = {'doc':'\\n'.join(sents),'labels':'\\n'.join(labels),'summaries':'\\n'.join(summaries)}\n",
        "        examples.append(ex)\n",
        "    return examples\n",
        "\n",
        "def build_dataset(args):\n",
        "    t1 = time()\n",
        "\n",
        "    print('start building dataset')\n",
        "    if args.worker_num == 1 and cpu_count() > 1:\n",
        "        print('[INFO] There are %d CPUs in your device, please increase -worker_num to speed up' % (cpu_count()))\n",
        "        print(\"       It's a IO intensive application, so 2~10 may be a good choise\")\n",
        "\n",
        "    files = glob(args.source_dir)\n",
        "    data_num = len(files)\n",
        "    group_size = data_num // args.worker_num\n",
        "    groups = []\n",
        "    for i in range(args.worker_num):\n",
        "        if i == args.worker_num - 1:\n",
        "            groups.append(files[i*group_size : ])\n",
        "        else:\n",
        "            groups.append(files[i*group_size : (i+1)*group_size])\n",
        "    p = Pool(processes=args.worker_num)\n",
        "    multi_res = [p.apply_async(worker,(fs,)) for fs in groups]\n",
        "    res = [res.get() for res in multi_res]\n",
        "\n",
        "    with open(args.target_dir, 'w') as f:\n",
        "        for row in chain(*res):\n",
        "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    t2 = time()\n",
        "    print('Time Cost : %.1f seconds' % (t2 - t1))\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('-build_vocab',action='store_true')\n",
        "    parser.add_argument('-embed', type=str, default='data/100.w2v')\n",
        "    parser.add_argument('-vocab', type=str, default='data/embedding.npz')\n",
        "    parser.add_argument('-word2id',type=str,default='data/word2id.json')\n",
        "\n",
        "    parser.add_argument('-worker_num',type=int,default=1)\n",
        "    parser.add_argument('-source_dir', type=str, default='data/neuralsum/dailymail/validation/*')\n",
        "    parser.add_argument('-target_dir', type=str, default='data/val.json')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.build_vocab:\n",
        "        build_vocab(args)\n",
        "    else:\n",
        "        build_dataset(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [-build_vocab] [-embed EMBED] [-vocab VOCAB]\n",
            "                                [-word2id WORD2ID] [-worker_num WORKER_NUM]\n",
            "                                [-source_dir SOURCE_DIR] [-target_dir TARGET_DIR]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-9bd0326e-fc4e-41aa-b5f8-57c2fb671dc3.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJs1H_xIVS0s"
      },
      "source": [
        "# Main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAVUUYr3VTH5"
      },
      "source": [
        "import json\n",
        "import models\n",
        "import utils\n",
        "import argparse,random,logging,numpy,os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils import clip_grad_norm\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s [INFO] %(message)s')\n",
        "parser = argparse.ArgumentParser(description='extractive summary')\n",
        "# model\n",
        "parser.add_argument('-save_dir',type=str,default='checkpoints/')\n",
        "parser.add_argument('-embed_dim',type=int,default=100)\n",
        "parser.add_argument('-embed_num',type=int,default=100)\n",
        "parser.add_argument('-pos_dim',type=int,default=50)\n",
        "parser.add_argument('-pos_num',type=int,default=100)\n",
        "parser.add_argument('-seg_num',type=int,default=10)\n",
        "parser.add_argument('-kernel_num',type=int,default=100)\n",
        "parser.add_argument('-kernel_sizes',type=str,default='3,4,5')\n",
        "parser.add_argument('-model',type=str,default='RNN_RNN')\n",
        "parser.add_argument('-hidden_size',type=int,default=200)\n",
        "# train\n",
        "parser.add_argument('-lr',type=float,default=1e-3)\n",
        "parser.add_argument('-batch_size',type=int,default=32)\n",
        "parser.add_argument('-epochs',type=int,default=5)\n",
        "parser.add_argument('-seed',type=int,default=1)\n",
        "parser.add_argument('-train_dir',type=str,default='data/train.json')\n",
        "parser.add_argument('-val_dir',type=str,default='data/val.json')\n",
        "parser.add_argument('-embedding',type=str,default='data/embedding.npz')\n",
        "parser.add_argument('-word2id',type=str,default='data/word2id.json')\n",
        "parser.add_argument('-report_every',type=int,default=1500)\n",
        "parser.add_argument('-seq_trunc',type=int,default=50)\n",
        "parser.add_argument('-max_norm',type=float,default=1.0)\n",
        "# test\n",
        "parser.add_argument('-load_dir',type=str,default='checkpoints/RNN_RNN_seed_1.pt')\n",
        "parser.add_argument('-test_dir',type=str,default='data/test.json')\n",
        "parser.add_argument('-ref',type=str,default='outputs/ref')\n",
        "parser.add_argument('-hyp',type=str,default='outputs/hyp')\n",
        "parser.add_argument('-filename',type=str,default='x.txt') # TextFile to be summarized\n",
        "parser.add_argument('-topk',type=int,default=15)\n",
        "# device\n",
        "parser.add_argument('-device',type=int)\n",
        "# option\n",
        "parser.add_argument('-test',action='store_true')\n",
        "parser.add_argument('-debug',action='store_true')\n",
        "parser.add_argument('-predict',action='store_true')\n",
        "args = parser.parse_args()\n",
        "use_gpu = args.device is not None\n",
        "\n",
        "if torch.cuda.is_available() and not use_gpu:\n",
        "    print(\"WARNING: You have a CUDA device, should run with -device 0\")\n",
        "\n",
        "# set cuda device and seed\n",
        "if use_gpu:\n",
        "    torch.cuda.set_device(args.device)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "random.seed(args.seed)\n",
        "numpy.random.seed(args.seed)\n",
        "\n",
        "def eval(net,vocab,data_iter,criterion): #vocab class,data_iter(train or valid ITER), criterion=NN.bceloss\n",
        "    net.eval()\n",
        "    total_loss = 0\n",
        "    batch_num = 0\n",
        "    for batch in data_iter:\n",
        "        features,targets,_,doc_lens = vocab.make_features(batch)\n",
        "        features,targets = Variable(features), Variable(targets.float())\n",
        "        if use_gpu:\n",
        "            features = features.cuda()\n",
        "            targets = targets.cuda()\n",
        "        probs = net(features,doc_lens)\n",
        "        loss = criterion(probs,targets)\n",
        "        total_loss += loss.data[0]\n",
        "        batch_num += 1\n",
        "    loss = total_loss / batch_num\n",
        "    net.train()\n",
        "    return loss\n",
        "\n",
        "def train():\n",
        "    logging.info('Loading vocab,train and val dataset.Wait a second,please')\n",
        "\n",
        "    embed = torch.Tensor(np.load(args.embedding)['embedding'])\n",
        "    with open(args.word2id) as f:\n",
        "        word2id = json.load(f)\n",
        "    vocab = utils.Vocab(embed, word2id) #passing word2id and embedding files to vocab class in utils folder\n",
        "\n",
        "    with open(args.train_dir) as f:\n",
        "        examples = [json.loads(line) for line in f]\n",
        "    train_dataset = utils.Dataset(examples) #passing training data to dataset class in utils folder\n",
        "\n",
        "    with open(args.val_dir) as f:\n",
        "        examples = [json.loads(line) for line in f]\n",
        "    val_dataset = utils.Dataset(examples) #passing valid dataset to dataset class in utils folder\n",
        "\n",
        "    # update args\n",
        "    args.embed_num = embed.size(0)\n",
        "    args.embed_dim = embed.size(1)\n",
        "    args.kernel_sizes = [int(ks) for ks in args.kernel_sizes.split(',')] #taking kernel sizes as list [3,4,5]\n",
        "    # build model\n",
        "    net = getattr(models,args.model)(args,embed)\n",
        "#The getattr() method returns the value of the named attribute of an object. If not found, it returns the default value provided to the function.\n",
        "    if use_gpu:\n",
        "        net.cuda()\n",
        "    # load dataset\n",
        "#The DataLoader class is designed so that it can be iterated using the enumerate() function, which returns a tuple with the current batch zero-based index value, and the actual batch of data.\n",
        "    train_iter = DataLoader(dataset=train_dataset,\n",
        "            batch_size=args.batch_size,\n",
        "            shuffle=True)\n",
        "    val_iter = DataLoader(dataset=val_dataset,\n",
        "            batch_size=args.batch_size,\n",
        "            shuffle=False)\n",
        "    # loss function\n",
        "    criterion = nn.BCELoss()\n",
        "#binary cross entropy\n",
        "#BCELoss creates a criterion that measures the Binary Cross Entropy between the target and the output. You can read more about BCELoss here. If we use BCELoss function we need to have a sigmoid layer in our network.\n",
        "#Also called Softmax Loss. It is a Softmax activation plus a Cross-Entropy loss. If we use this loss, we will train a CNN to output a probability over the C classes for each image. It is used for multi-class classification.\n",
        "\n",
        "    # model info\n",
        "    print(net)\n",
        "    params = sum(p.numel() for p in list(net.parameters())) / 1e6\n",
        "    print('#Params: %.1fM' % (params))\n",
        "\n",
        "    min_loss = float('inf')\n",
        "#float('inf') is used for setting a variable with an infinitely large value. In simple words, it sets the value as +ve infinty.\n",
        "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr)\n",
        "    net.train()\n",
        "\n",
        "    t1 = time()\n",
        "    for epoch in range(1,args.epochs+1):\n",
        "        for i,batch in enumerate(train_iter): #train_iter is like (1,batch)\n",
        "            features,targets,_,doc_lens = vocab.make_features(batch)\n",
        "            features,targets = Variable(features), Variable(targets.float())\n",
        "            if use_gpu:\n",
        "                features = features.cuda()\n",
        "                targets = targets.cuda()\n",
        "            probs = net(features,doc_lens)\n",
        "            loss = criterion(probs,targets)\n",
        "#Criterions are helpful to train a neural network. Given an input and a target, they compute a gradient according to a given loss function\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            clip_grad_norm(net.parameters(), args.max_norm)\n",
        "            optimizer.step()\n",
        "            if args.debug:\n",
        "                print('Batch ID:%d Loss:%f' %(i,loss.data[0]))\n",
        "                continue\n",
        "            if i % args.report_every == 0:\n",
        "                cur_loss = eval(net,vocab,val_iter,criterion)\n",
        "                if cur_loss < min_loss:\n",
        "                    min_loss = cur_loss\n",
        "                    best_path = net.save()\n",
        "                logging.info('Epoch: %2d Min_Val_Loss: %f Cur_Val_Loss: %f'\n",
        "                        % (epoch,min_loss,cur_loss))\n",
        "    t2 = time()\n",
        "    logging.info('Total Cost:%f h'%((t2-t1)/3600))\n",
        "\n",
        "def test():\n",
        "\n",
        "    embed = torch.Tensor(np.load(args.embedding)['embedding'])\n",
        "    with open(args.word2id) as f:\n",
        "        word2id = json.load(f)\n",
        "    vocab = utils.Vocab(embed, word2id)\n",
        "\n",
        "    with open(args.test_dir, encoding=\"mbcs\") as f:\n",
        "        examples = [json.loads(line) for line in f]\n",
        "    test_dataset = utils.Dataset(examples)\n",
        "\n",
        "    test_iter = DataLoader(dataset=test_dataset,\n",
        "                            batch_size=args.batch_size,\n",
        "                            shuffle=False)\n",
        "    if use_gpu:\n",
        "        checkpoint = torch.load(args.load_dir) #loads the model\n",
        "    else:\n",
        "        checkpoint = torch.load(args.load_dir, map_location=lambda storage, loc: storage)\n",
        "\n",
        "    # checkpoint['args']['device'] saves the device used as train time\n",
        "    # if at test time, we are using a CPU, we must override device to None\n",
        "    if not use_gpu:\n",
        "        checkpoint['args'].device = None\n",
        "    net = getattr(models,checkpoint['args'].model)(checkpoint['args'])\n",
        "    net.load_state_dict(checkpoint['model'])\n",
        "    if use_gpu:\n",
        "        net.cuda()\n",
        "    net.eval()\n",
        "\n",
        "    doc_num = len(test_dataset)\n",
        "    time_cost = 0\n",
        "    file_id = 1\n",
        "    for batch in tqdm(test_iter): #Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable)\n",
        "        features,_,summaries,doc_lens = vocab.make_features(batch)\n",
        "        t1 = time()\n",
        "        if use_gpu:\n",
        "            probs = net(Variable(features).cuda(), doc_lens) #here variable is what links the data with the model\n",
        "        else:\n",
        "            probs = net(Variable(features), doc_lens)\n",
        "        t2 = time()\n",
        "        time_cost += t2 - t1\n",
        "        start = 0\n",
        "        for doc_id,doc_len in enumerate(doc_lens):\n",
        "            stop = start + doc_len\n",
        "            prob = probs[start:stop]\n",
        "            topk = min(args.topk,doc_len) #condidering 15 topk by default in args\n",
        "            topk_indices = prob.topk(topk)[1].cpu().data.numpy()\n",
        "            topk_indices.sort()\n",
        "            doc = batch['doc'][doc_id].split('\\n')[:doc_len]\n",
        "            hyp = [doc[index] for index in topk_indices]\n",
        "            ref = summaries[doc_id]\n",
        "            with open(os.path.join(args.ref,str(file_id)+'.txt'), 'w') as f:\n",
        "                f.write(ref)\n",
        "            with open(os.path.join(args.hyp,str(file_id)+'.txt'), 'w', encoding=\"mbcs\") as f:\n",
        "                f.write('\\n'.join(hyp))\n",
        "            start = stop\n",
        "            file_id = file_id + 1\n",
        "    print('Speed: %.2f docs / s' % (doc_num / time_cost))\n",
        "\n",
        "\n",
        "def predict(examples):\n",
        "    embed = torch.Tensor(np.load(args.embedding)['embedding'])\n",
        "    with open(args.word2id) as f:\n",
        "        word2id = json.load(f)\n",
        "    vocab = utils.Vocab(embed, word2id)\n",
        "    pred_dataset = utils.Dataset(examples)\n",
        "\n",
        "    pred_iter = DataLoader(dataset=pred_dataset,\n",
        "                            batch_size=args.batch_size,\n",
        "                            shuffle=False)\n",
        "    if use_gpu:\n",
        "        checkpoint = torch.load(args.load_dir)\n",
        "    else:\n",
        "        checkpoint = torch.load(args.load_dir, map_location=lambda storage, loc: storage)\n",
        "\n",
        "    # checkpoint['args']['device'] saves the device used as train time\n",
        "    # if at test time, we are using a CPU, we must override device to None\n",
        "    if not use_gpu:\n",
        "        checkpoint['args'].device = None\n",
        "    net = getattr(models,checkpoint['args'].model)(checkpoint['args'])\n",
        "    net.load_state_dict(checkpoint['model'])\n",
        "    if use_gpu:\n",
        "        net.cuda()\n",
        "    net.eval()\n",
        "\n",
        "    doc_num = len(pred_dataset)\n",
        "    time_cost = 0\n",
        "    file_id = 1\n",
        "    for batch in tqdm(pred_iter):\n",
        "        features, doc_lens = vocab.make_predict_features(batch)\n",
        "        t1 = time()\n",
        "        if use_gpu:\n",
        "            probs = net(Variable(features).cuda(), doc_lens)\n",
        "        else:\n",
        "            probs = net(Variable(features), doc_lens)\n",
        "        t2 = time()\n",
        "        time_cost += t2 - t1\n",
        "        start = 0\n",
        "        for doc_id,doc_len in enumerate(doc_lens):\n",
        "            stop = start + doc_len\n",
        "            prob = probs[start:stop]\n",
        "            topk = min(args.topk,doc_len)\n",
        "            topk_indices = prob.topk(topk)[1].cpu().data.numpy()\n",
        "            topk_indices.sort()\n",
        "            doc = batch[doc_id].split('. ')[:doc_len]\n",
        "            hyp = [doc[index] for index in topk_indices]\n",
        "            with open(os.path.join(args.hyp,str(file_id)+'.txt'), 'w') as f:\n",
        "                f.write('. '.join(hyp))\n",
        "            start = stop\n",
        "            file_id = file_id + 1\n",
        "    print('Speed: %.2f docs / s' % (doc_num / time_cost))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    if args.test:\n",
        "        test()\n",
        "    elif args.predict:\n",
        "        with open(args.filename) as file:\n",
        "            bod = [file.read()]\n",
        "        predict(bod)\n",
        "    else:\n",
        "        train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZweW9nh2Sw"
      },
      "source": [
        "# Outputs folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0c4bY3nWQFs"
      },
      "source": [
        "# Rouge_score.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NTVAMZRWPY8"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
        "avg_precision1,avg_recall1,avg_fscore1=0,0,0\n",
        "avg_precision2,avg_recall2,avg_fscore2=0,0,0\n",
        "avg_precisionl,avg_recalll,avg_fscorel=0,0,0\n",
        "tot=300\n",
        "for i in range(1,tot+1):\n",
        "    try:\n",
        "        sys_gen = ''.join(open('./hyp/'+str(i)+'.txt').readlines())\n",
        "        text = ''.join(open('./ref/'+str(i)+'.txt').readlines())\n",
        "#         [precision, recall, f_score] = r.rouge_l([sys_gen], [text])\n",
        "#         avg_precision+=precision\n",
        "#         avg_fscore+=f_score\n",
        "#         avg_recall+=recall\n",
        "        score=scorer.score(sys_gen,text)\n",
        "        avg_precision1+=score['rouge1'].precision\n",
        "        avg_precision2+=score['rouge2'].precision\n",
        "        avg_precisionl+=score['rougeL'].precision\n",
        "\n",
        "        avg_recall1+=score['rouge1'].recall\n",
        "        avg_recall2+=score['rouge2'].recall\n",
        "        avg_recalll+=score['rougeL'].recall\n",
        "\n",
        "        avg_fscore1+=score['rouge1'].fmeasure\n",
        "        avg_fscore2+=score['rouge2'].fmeasure\n",
        "        avg_fscorel+=score['rougeL'].fmeasure\n",
        "#         scrs.append(scorer.score(sys_gen,text))\n",
        "    except:\n",
        "        tot-=1\n",
        "print(avg_precision1/tot,avg_recall1/tot,avg_fscore1/tot)\n",
        "print(avg_precision2/tot,avg_recall2/tot,avg_fscore2/tot)\n",
        "print(avg_precisionl/tot,avg_recalll/tot,avg_fscorel/tot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JowSrQDymJ3s"
      },
      "source": [
        "arr=torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
        "print(arr.size(1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}