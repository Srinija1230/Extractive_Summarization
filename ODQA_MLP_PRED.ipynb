{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "quyyUQGioZnQ"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import spacy\n",
        "def que_classification():\n",
        "    f=open(\"/content/drive/My Drive/final_intern/predefined.txt\",\"r\",encoding='utf-8')\n",
        "    classes=[] #all the labels of the questions\n",
        "    query=[]   #queries corresponding to the classes\n",
        "    new_label=[] #labels in numerical representation\n",
        "    for line in f:\n",
        "        line=line.rstrip('\\n')\n",
        "        classes.append((line.split()[0]).split(\":\")[0])  #appending labels from the test to classes list\n",
        "        query.append(line[5:])                           #appending queries from text to query list\n",
        "    j=list(set(classes))   #set of the labels\n",
        "    k=[i for i in range(len(j))] #numbers for labels\n",
        "    my_dict={}\n",
        "    for i in range(7):\n",
        "        my_dict.update({j[i]:k[i]}) #dictionary {label:number}\n",
        "    for i in classes:\n",
        "        new_label.append(my_dict[i]) #labels in numeric representation\n",
        "    #query.append(given_query)\n",
        "    #vectorising all the inputs\n",
        "    vectorizer =TfidfVectorizer()\n",
        "    #X = vectorizer.fit_transform(classes) #Learn vocabulary and idf, return document-term matrix (input-raw_document)\n",
        "    Y = vectorizer.fit_transform(query)\n",
        "    z=Y.toarray()\n",
        "    #print(Y[-1])\n",
        "    X = z[:len(new_label)]#y[:1342]\n",
        "    y = new_label[:len(new_label)]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1250)\n",
        "\n",
        "    lr = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(14,), random_state=1)\n",
        "\n",
        "    lr.fit(X_train,y_train)\n",
        "    #print(\"Accuracy\")\n",
        "    return (vectorizer,lr,my_dict)\n",
        "def predict_ques(que):\n",
        "    #withbias=list(z[-1]) #question\n",
        "    #withbias.insert(0,1) # adding bias at front\n",
        "    query=[]\n",
        "    query.append(que)\n",
        "    withbias=vectorizer.transform(query).toarray()\n",
        "    maxi= lr.predict(withbias)\n",
        "    for i in my_dict:\n",
        "            if my_dict[i]==maxi:\n",
        "                print(\"\\nLabel of the given query :\",i)\n",
        "                break\n",
        "    return i\n",
        "(vectorizer,lr,my_dict)=que_classification()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8de8FnZ4oZnb"
      },
      "source": [
        "import re, math\n",
        "import urllib\n",
        "from googletrans import Translator\n",
        "import urllib3\n",
        "import warnings\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import inflect\n",
        "p = inflect.engine()\n",
        "import string\n",
        "import os\n",
        "java_path = \"C:/Program Files/Java/jdk1.8.0_151/bin/java.exe\"\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "prelink=\"https://www.bing.com/search?\"\n",
        "#from urllib.request import urlopen\n",
        "def preprocess(x):\n",
        "    # Step1 to lowercase\n",
        "    lowercase=x.lower()\n",
        "    temp_str = lowercase.split()\n",
        "    # Step2 removing digits\n",
        "    new_string = []\n",
        "    p = inflect.engine()\n",
        "    for word in temp_str:\n",
        "        # if word is a digit, convert the digit\n",
        "        # to numbers and append into the new_string list\n",
        "        if word.isdigit():\n",
        "            temp = p.number_to_words(word)\n",
        "            new_string.append(temp)\n",
        "\n",
        "        # append the word as it is\n",
        "        else:\n",
        "            new_string.append(word)\n",
        "\n",
        "    # join the words of new_string to form a string\n",
        "    temp_str = ' '.join(new_string)\n",
        "    # Step-3 removing punctuations\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    temp_str = temp_str.translate(translator)\n",
        "    # Step-4 removing stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    word_tokens = word_tokenize(temp_str)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    filtered_text =' '.join(filtered_text)\n",
        "    # step-5 Lemmatise\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    word_tokens = word_tokenize(filtered_text)\n",
        "    # provide context i.e. part-of-speech\n",
        "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "    return lemmas\n",
        "def NER_With_corenlp(qc):\n",
        "    print(\"qc :\",qc)\n",
        "    from pycorenlp import StanfordCoreNLP\n",
        "    nlp = StanfordCoreNLP('http://localhost:9000')\n",
        "    Possible_answers = []\n",
        "    All_answers      = []\n",
        "    Ner_Entites_Dict ={'PERSON':[],'ORG':[],'CARDINAL':[],'TIME':[],'DATE':[],'GPE':[],'PERCENT':[],'NORP':[],'ORDINAL':[],'PRODUCT':[],'LAW':[]}\n",
        "    for i in range(len(Top_similarity_sent)):\n",
        "        while(1):\n",
        "            try:\n",
        "                result = nlp.annotate(Top_similarity_sent[i],\n",
        "                           properties={\n",
        "                               'annotators': 'sentiment, ner, pos',\n",
        "                               'outputFormat': 'json',\n",
        "                               'timeout': 1000,\n",
        "                           })\n",
        "                for word in result[\"sentences\"][0]['tokens']:\n",
        "                    Ner_Entites_Dict[word['word']]=word['ner']\n",
        "                break\n",
        "            except TypeError as t:\n",
        "                print(\"TimeOut- Restarting\")\n",
        "    if qc in Ner_Entites_Dict.keys():\n",
        "        return Ner_Entites_Dict[qc]\n",
        "\n",
        "\n",
        "def NER_other_way(qc):\n",
        "    print(\"qc : \",qc)\n",
        "\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    #stanford_ner_tagger = StanfordNERTagger('C:/Users/B151119/Desktop/SummerInternship/Week-5/stanford-ner-4.0.0/classifiers/english.muc.7class.distsim.crf.ser.gz','C:/Users/B151119/Desktop/SummerInternship/Week-5/stanford-ner-4.0.0/stanford-ner.jar')\n",
        "    ners1 ={'PERSON':[],'ORG':[],'CARDINAL':[],'TIME':[],'DATE':[],'GPE':[],'PERCENT':[],'LAW':[],'ORDINAL'\n",
        "        :[],'NORP':[],'LOC':[],'EVENT':[],'FAC':[],'QUANTITY':[],'MONEY':[]}\n",
        "    for i in range(len(Top_similarity_sent)):\n",
        "        sentence=Top_similarity_sent[i]\n",
        "        doc = nlp(sentence)\n",
        "        for ent in doc.ents:\n",
        "            if(ent.label_ in ners1.keys()):\n",
        "                ners1[ent.label_].append(ent.text)\n",
        "    print(ners1)\n",
        "    ners=ners1\n",
        "    ners['LOCA'] = ners.pop('GPE')\n",
        "    ners['ORGA'] = ners.pop('ORG')\n",
        "    ners['NUMB'] = ners.pop('CARDINAL')\n",
        "    ners['PERC'] = ners.pop('PERCENT')\n",
        "    ners['PERS'] = ners.pop('PERSON')\n",
        "    ners['CUR'] = ners.pop('MONEY')\n",
        "    ners['QUANT']=ners.pop('QUANTITY')\n",
        "    '''\n",
        "    lis = ners['TIME']\n",
        "    for i in lis:\n",
        "        ners['NUMB'].append(i)\n",
        "    lis = ners['PERC']\n",
        "    for i in lis:\n",
        "        ners['NUMB'].append(i)\n",
        "\n",
        "    '''\n",
        "    lis = ners['LOC']\n",
        "    for i in lis:\n",
        "        ners['LOCA'].append(i)\n",
        "    #lis = ners['NORP']\n",
        "    #for i in lis:\n",
        "        #ners['PERS'].append(i)\n",
        "    lis = ners['ORDINAL']\n",
        "    for i in lis:\n",
        "        ners['NUMB'].append(i)\n",
        "    return (ners1,ners[qc])\n",
        "\n",
        "    #print(\"Possible_answers : \",Possible_answers)\n",
        "    #print(\"\\n\\n\")\n",
        "    #print(\"All_answers : \",All_answers)\n",
        "    #print(\"\\n\\n\")\n",
        "    #return Possible_answers\n",
        "\n",
        "def qquery(qu):\n",
        "    # BING\n",
        "    print(\"BING SEARCH\")\n",
        "    address = \"http://www.bing.com/search?q=%s\" % (urllib.parse.quote_plus(str(qu)))\n",
        "    http = urllib3.PoolManager()\n",
        "    resp = http.request('GET',address)\n",
        "    soup = BeautifulSoup(resp.data)\n",
        "    print(\"Top 10 URLS:\\n----------------------------------------------------------------\\n\\n\")\n",
        "    if not(soup.findAll('li', attrs={'class':'b_algo'})):\n",
        "        print(\"Not able to find urls, Some Error came call back the function again....\\n\")\n",
        "        qquery(qu)\n",
        "\n",
        "    for listt in soup.findAll('li', attrs={'class':'b_algo'}):\n",
        "        head_links = listt.find('h2')\n",
        "        for link in head_links.find_all('a', href=True):\n",
        "            Top_10_urls.append(link['href'])\n",
        "            print(link['href']+\"\\n\")\n",
        "            Whole_output.append(link['href']+\"\\n\")\n",
        "    #print(\"OK returning\")\n",
        "    #GOOGLE\n",
        "    '''\n",
        "    print(\"GOOGLE\")\n",
        "    try:\n",
        "        from googlesearch import search\n",
        "    except ImportError:\n",
        "        print(\"No module named 'google' found\")\n",
        "    for j in search(qu, tld=\"co.in\", num=10, stop=10, pause=2):\n",
        "        Top_10_urls.append(j)\n",
        "        print(j)\n",
        "    '''\n",
        "\n",
        "    return Top_10_urls\n",
        "\n",
        "\n",
        "def translate(to_translate, to_langage=\"auto\", langage=\"auto\"):\n",
        "    try:\n",
        "        translator = Translator()\n",
        "        result = translator.translate(to_translate, src='te', dest='en')\n",
        "        quest.append(result.text)\n",
        "        Whole_output.append(result.text)\n",
        "        return result\n",
        "    except:\n",
        "        print(\"Translate error\")\n",
        "        return translate(to_translate, to_langage=\"auto\", langage=\"auto\")\n",
        "\n",
        "def translate2(to_translate, to_langage=\"auto\", langage=\"auto\"):\n",
        "    import goslate\n",
        "    gs = goslate.Goslate()\n",
        "    print(gs.translate(to_translate, 'en'))\n",
        "    result = gs.translate(to_translate, 'en')\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVPRVeIToZni"
      },
      "source": [
        "def Sentence_Extraction():\n",
        "    Numlinks=len(Top_10_urls)\n",
        "    #sen=open(\"Whole_sentence.txt\",\"w\")\n",
        "    i=0\n",
        "    for i in range(Numlinks):\n",
        "        try:\n",
        "            nP=0\n",
        "            http = urllib3.PoolManager()\n",
        "            resp = http.request('GET',Top_10_urls[i])\n",
        "            #content=urllib2.urlopen(Top_10_urls[i]) ############ May be Here you may get ERROR\n",
        "            htmlSource=resp.data\t############ May be Here you may get ERROR\n",
        "            contentt = BeautifulSoup(htmlSource)\n",
        "\n",
        "            for para in contentt.findAll(['p']):\n",
        "                pa=para.text\n",
        "                whole_sent.append(pa.strip()) #all sentence writing into one list line by line\n",
        "                #sen.write(pa.encode('UTF-8')+\"\\n\") # all sentence writing into one file line by line\n",
        "                nP=nP+1 # number of imp content paragraphs in each URL...........\n",
        "        except IOError as e:\n",
        "            print(\" IO ERROR \")\n",
        "        except ValueError:\n",
        "            print(\"VALUE ERROR \")\n",
        "        except:\n",
        "            print(\"ERROR \")\n",
        "            Top_10_urls.pop(i)\n",
        "            Sentence_Extraction()\n",
        "            break\n",
        "\n",
        "    #sen.close()\n",
        "    #print(\"\\n\\nTotal extracted lines :\"+str(len(whole_sent))+\"\\n\\n\")\n",
        "\n",
        "\n",
        "def Finding_Similarity_Matrix(q):\n",
        "    cc=0\n",
        "    for i in range(len(whole_sent)):\n",
        "        stmt=whole_sent[i].split(\".\")\n",
        "        for j in range(len(stmt)):\n",
        "            if(len(stmt[j].split(\"?\"))==1):\n",
        "                cc=cc+1\n",
        "                all_sen.append(stmt[j])\n",
        "    all_sen1=list(set(all_sen))\n",
        "    WORD = re.compile(r'\\w+')\n",
        "    def get_cosine(vec1, vec2):\n",
        "        intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "        numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "        sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "        sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "        if not denominator:\n",
        "            return(0.0)\n",
        "        else:\n",
        "            return(float(numerator) / denominator)\n",
        "\n",
        "    def text_to_vector(text):\n",
        "        words = WORD.findall(text)\n",
        "        return(Counter(words))\n",
        "    text1 =q  # given user query\n",
        "    x=0\n",
        "\n",
        "    for n in range(len(all_sen)):\n",
        "        text2=all_sen[n]\n",
        "        vector1 = text_to_vector(text1)\n",
        "        vector2 = text_to_vector(text2)\n",
        "\n",
        "        cosine = get_cosine(vector1, vector2)\n",
        "        x=x+1\n",
        "        consine_list.append(cosine)\n",
        "\n",
        "\n",
        "def Most_Relative_Sentence():\n",
        "\n",
        "\tnum=10 ##number of most relative sentences considering\n",
        "\tif(len(whole_sent)<num):\n",
        "\t\tnum = len(whole_sent)\n",
        "\ttt= (sorted(consine_list, reverse=True)[:num])\n",
        "\tfor k in range(num):\n",
        "\t\tl=[i for i,j in enumerate(consine_list) if j==tt[k]]\n",
        "\t\tmyString = all_sen[l[0]].strip()\n",
        "\t\tsent = ' '.join(myString.split())\n",
        "\t\tTop_similarity_sent.append(sent)\n",
        "\t\tWhole_output.append(sent+\"\\n\")\n",
        "\treturn Top_similarity_sent\n",
        "\n",
        "def que_ner(qc_label,sent_list):\n",
        "    ners1 ={'PERSON':[],'ORG':[],'CARDINAL':[],'TIME':[],'DATE':[],'GPE':[],'PERCENT':[],'LAW':[],'ORDINAL'\n",
        "        :[],'NORP':[],'LOC':[],'EVENT':[],'FAC':[],'QUANTITY':[],'MONEY':[]}\n",
        "    for i in range(len(sent_list)):\n",
        "        sentence=sent_list[i]\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        doc = nlp(sentence)\n",
        "        for ent in doc.ents:\n",
        "            if(ent.label_ in ners1.keys()):\n",
        "                ners1[ent.label_].append(ent.text)\n",
        "    ners=ners1\n",
        "    print(ners)\n",
        "    ners['LOCA'] = ners.pop('GPE')\n",
        "    ners['ORGA'] = ners.pop('ORG')\n",
        "    ners['NUMB'] = ners.pop('CARDINAL')\n",
        "    ners['PERC'] = ners.pop('PERCENT')\n",
        "    ners['PERS'] = ners.pop('PERSON')\n",
        "    ners['CUR'] = ners.pop('MONEY')\n",
        "    ners['QUANT']=ners.pop('QUANTITY')\n",
        "    lis = ners['LOC']\n",
        "    for i in lis:\n",
        "        ners['LOCA'].append(i)\n",
        "    #lis = ners['NORP']\n",
        "    #for i in lis:\n",
        "        #ners['PERS'].append(i)\n",
        "    lis = ners['ORDINAL']\n",
        "    for i in lis:\n",
        "        ners['NUMB'].append(i)\n",
        "    for i in range(len(ners[qc_label])):\n",
        "        ners[qc_label][i]=ners[qc_label][i].lower()\n",
        "    result=ners[qc_label]\n",
        "    return (ners1,result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tvklSJ4foZnr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "655c790c-625b-4fb7-d6e5-31a0ef77ebe1"
      },
      "source": [
        "############################################# Calling QA System Step by Step #########################################################\n",
        "import pandas as pd\n",
        "import time\n",
        "input_file = open(\"/content/drive/My Drive/final_intern/updated_file_847_que\",\"r\",encoding='utf-8').readlines()\n",
        "Questions = []\n",
        "actual_ans = []\n",
        "final_ans = []\n",
        "labels=[]\n",
        "#####FILE OPENED IN WRITE MODE#####\n",
        "file1 = open('myfile.txt', 'w', encoding='utf-8')\n",
        "def findingmostsimilar(poss , similar):\n",
        "    besind=100\n",
        "    best=[]\n",
        "    for j in poss:\n",
        "        for i in range(len(similar)):\n",
        "            tok = similar[i].split(\" \")\n",
        "            if(j in tok):\n",
        "                if(besind>=i):\n",
        "                    besind=i\n",
        "                    best.append(j)\n",
        "    return best[0]\n",
        "def ners(ner):\n",
        "    possibles =[]\n",
        "    for i in ner.keys():\n",
        "        for j in ner[i]:\n",
        "            possibles.append(j)\n",
        "    return possibles\n",
        "def find_max_freq(que_ner,ner,Top_similarity_sent):\n",
        "    possibles = ners(ner)\n",
        "    que_ners = ners(que_ner)\n",
        "    for i in ner.keys():\n",
        "        for j in ner[i]:\n",
        "            possibles.append(j)\n",
        "    words, counts = np.unique(possibles, return_counts=True)\n",
        "    print(\"DATAFRAME WITH COUNT:\")\n",
        "    corpus_df = pd.DataFrame({'WORD':words, 'COUNT':counts})\n",
        "    corpus_df.sort_values('COUNT', ascending=False, inplace=True)\n",
        "    corpus_df.reset_index(drop=True,inplace=True)\n",
        "    find=corpus_df[corpus_df.columns[0]]\n",
        "    print(find)\n",
        "    samefreq=[]\n",
        "    maxfreq=0\n",
        "    for i in range(len(find)):\n",
        "        if(find[i].lower() not in que_ners):\n",
        "            found=1\n",
        "            freq=int(corpus_df[corpus_df['WORD']==find[i]]['COUNT'])\n",
        "            if(maxfreq<=freq):\n",
        "                samefreq.append(find[i])\n",
        "                maxfreq=freq\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    if(len(samefreq)==1):\n",
        "        return samefreq[0]\n",
        "    elif(len(samefreq)>1):\n",
        "        return findingmostsimilar(samefreq,Top_similarity_sent)\n",
        "    return \"NO ANSWER\"\n",
        "def cleanup_funct():\n",
        "\tTop_10_urls=[]\n",
        "\twhole_sent=[]\n",
        "\tconsine_list=[]\n",
        "\tall_sen=[]\n",
        "\tTop_similarity_sent=[]\n",
        "\tans=[]\n",
        "\tSentence_NER=[]\n",
        "\tner=[]\n",
        "\tquest=[]\n",
        "\tq_ner=[]\n",
        "\tq_text=[]\n",
        "\tWhole_output=[]\n",
        "\tfinal_answers=[]\n",
        "\tss=[]\n",
        "\tlabel_ans=[]\n",
        "\tans_type=[]\n",
        "\tall_sen1=[]\n",
        "\trandom_ans=[]\n",
        "\tactual_ans=[]\n",
        "\treturn (Top_10_urls,whole_sent,consine_list,all_sen,Top_similarity_sent,ans,\n",
        "\t\t\tSentence_NER,ner,quest,q_ner,q_text,Whole_output,\n",
        "\t\t\tfinal_answers,label_ans,ss,ans_type,all_sen1, random_ans)\n",
        "for line in input_file:\n",
        "    #print(line)\n",
        "    line=line.strip()\n",
        "    line=line.split(\"@@@\")\n",
        "    labels.append(line[0])\n",
        "    actual_ans.append(line[1].split(\":::\")[0])\n",
        "    Questions.append(line[1].split(\":::\")[1])\n",
        "start=time.time()\n",
        "l=0        #initializing\n",
        "for l in range(845,847): #running from 0 to 186\n",
        "#qa=\"న్యూజిలాండ్ రాజధాని ఏమిటి?\"\n",
        "    qa=Questions[l]\n",
        "    print(\"\\n---------------------------------------------------------\\n\\n\")\n",
        "    print(\" QUESTION \")\n",
        "    print(l)\n",
        "    #for qa in Questions:\n",
        "    (Top_10_urls,whole_sent,consine_list,all_sen,Top_similarity_sent,ans,Sentence_NER,ner,quest,q_ner,q_text,Whole_output,final_answers,\n",
        "    label_ans,ss,ans_type,all_sen1,random_ans ) = cleanup_funct()\n",
        "    #answer=Question_Type(qa) # calling to get Rule Based Query Type\n",
        "    print(\"\\n---------------------------------------------------------\\n\\n\")\n",
        "    #print(\"Query Type:\\n\")\n",
        "    #print(answer)\n",
        "    q=translate(qa,'en','te') # translate te-->eng\n",
        "    print(\"\\n--------------------------------------------------------\\n\\n\")\n",
        "    print(\"Telugu Sentence:\\t\"+qa+\"\\n\")\n",
        "    print(\"English Sentence:\\t\"+q.text+\"\\n\")\n",
        "    print(\"\\n\\n\")\n",
        "    #print(\"Top 10 URLS:\\n----------------------------------------------------------------\\n\\n\")\n",
        "    Top_10_urls = qquery(q.text) # calling get Top 10 URLS\n",
        "    print(\"---------------------------------------------------------\\n\\n\")\n",
        "    Sentence_Extraction()  # calling to get whole content from 10 urls(well define sentences only)\n",
        "    Finding_Similarity_Matrix(q.text) # calling find similarity matri between query and all sentences\n",
        "    Top_similarity_sent = Most_Relative_Sentence() # calling to print higest top 5 similarity senteces.\n",
        "    print(\"Top_similarity_sent : \",Top_similarity_sent)\n",
        "    print(\"\\n---------------------------------------------------------\\n\\n\")\n",
        "    qc=predict_ques(qa) # user query question classification\n",
        "    #print(\"\\n Question Classification Prediction for User Query:\\n\")\n",
        "    print('qc :',qc)\n",
        "    (ner,possibles) = NER_other_way(qc)\n",
        "    print(\"\\n------------------------------------------------------------\\n\\n\")\n",
        "    print(\"possible answers:\")\n",
        "    print(possibles)\n",
        "    words, counts = np.unique(possibles, return_counts=True)\n",
        "    print(\"DATAFRAME WITH COUNT:\")\n",
        "    corpus_df = pd.DataFrame({'WORD':words, 'COUNT':counts})\n",
        "    corpus_df.sort_values('COUNT', ascending=False, inplace=True)\n",
        "    corpus_df.reset_index(drop=True,inplace=True)\n",
        "    find=corpus_df[corpus_df.columns[0]]\n",
        "    print(find)\n",
        "    dis = {'thrice':'three','twice':'two','once':'one','third':'three','second':'two','first':'one'}\n",
        "    #lower = preprocess(q.text)\n",
        "    (ner1,lower) = que_ner(qc, [q.text])\n",
        "    print(\"QUESTION TOKENIZING TO NEGLECT FEW ANSWERS\")\n",
        "    print(lower)\n",
        "    found=0\n",
        "    if(len(find)==0):\n",
        "        ans=find_max_freq(ner1,ner,Top_similarity_sent)\n",
        "        print(\"FINAL ANSWER\\n\")\n",
        "        print(ans)\n",
        "        file1.write(ans+\"\\n\")\n",
        "        final_ans.append(ans)\n",
        "    else:\n",
        "        for i in range(len(find)):\n",
        "            if(find[i] in dis.keys()):\n",
        "                find[i]=dis[find[i]]\n",
        "        samefreq=[]\n",
        "        maxfreq=0\n",
        "        for i in range(len(find)):\n",
        "            if(find[i].lower() not in lower):\n",
        "                found=1\n",
        "                freq=int(corpus_df[corpus_df['WORD']==find[i]]['COUNT'])\n",
        "                if(maxfreq<=freq):\n",
        "                    samefreq.append(find[i])\n",
        "                    maxfreq=freq\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        if(len(samefreq)==1):\n",
        "            print(\"FINAL ANSWER1:\")\n",
        "            print(samefreq[0])\n",
        "            file1.write(samefreq[0]+\"\\n\")\n",
        "            final_ans.append(samefreq[0])\n",
        "        elif(len(samefreq)>1):\n",
        "            ans=findingmostsimilar(samefreq,Top_similarity_sent)\n",
        "            print(\"FINAL ANSWER2:\")\n",
        "            print(ans)\n",
        "            file1.write(ans+\"\\n\")\n",
        "            final_ans.append(ans)\n",
        "\n",
        "        if(found==0):\n",
        "            ans=find_max_freq(ner1,ner,Top_similarity_sent)\n",
        "            print(\"FINAL ANSWER:\")\n",
        "            print(ans)\n",
        "            file1.write(ans+\"\\n\")\n",
        "            final_ans.append(ans)\n",
        "end=time.time()\n",
        "print(\"\\n-----------------------------------------------------------\\n\\n\")\n",
        "print(\"ACTUAL ANS\\n\")\n",
        "print(actual_ans)\n",
        "print(\"\\n-----------------------------------------------------------\\n\\n\")\n",
        "print(\"LABELS\\n\")\n",
        "print(labels)\n",
        "print(\"\\n-----------------------------------------------------------\\n\\n\")\n",
        "print(\"FINAL ANS\\n\")\n",
        "print(final_ans)\n",
        "print(\"TOTAL TIME FOR QUESTIONS in sec :\")\n",
        "print(end-start)\n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "\n",
            " QUESTION \n",
            "845\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "Telugu Sentence:\tమహిళల సింగిల్స్ ప్రపంచ బ్యాడ్మింటన్ ఛాంపియన్ షిప్ 2017 విజేత ఎవరు?\n",
            "\n",
            "English Sentence:\tWho is the winner of Women's Singles World Badminton Championship 2017?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "BING SEARCH\n",
            "Top 10 URLS:\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\n",
            "https://en.wikipedia.org/wiki/2017_BWF_World_Championships_%E2%80%93_Women%27s_singles\n",
            "\n",
            "https://en.wikipedia.org/wiki/2017_BWF_World_Championships\n",
            "\n",
            "https://www.queryhome.com/sports/4986/has-2017-womens-singles-world-badminton-championships-wbc\n",
            "\n",
            "https://www.queryhome.com/sports/5417/has-2017-womens-singles-national-badminton-championships\n",
            "\n",
            "https://www.sportshistori.com/2017/08/bwf-world-championships-winners.html\n",
            "\n",
            "https://www.youtube.com/watch?v=_JyL12e-a80\n",
            "\n",
            "https://en.wikipedia.org/wiki/Gold_medalists_at_the_BWF_World_Championships\n",
            "\n",
            "https://en.wikipedia.org/wiki/List_of_BWF_World_Championships_medalists\n",
            "\n",
            "https://economictimes.indiatimes.com/magazines/panache/how-badminton-star-pv-sindhu-carved-a-niche-for-herself/articleshow/76795235.cms\n",
            "\n",
            "https://en.wikipedia.org/wiki/BWF_World_Championships\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "\n",
            "Top_similarity_sent :  [\"The Women's Singles tournament of the 2017 BWF World Championships took place from 21 to 27 August\", \"Japan's Nozomi Okuhara has won the 2017 women’s singles World Badminton Championships (WBC)\", \"Japan's Nozomi Okuhara has won the 2017 women’s singles World Badminton Championships (WBC)\", \"India's Saina Nehwal has won bronze in the 2017 women’s singles World Badminton championships (WBC)\", 'The seeding list is based on the World Rankings of Thursday 3 August 2017', 'India’s Olympic 2016 silver medallist PV Sindhu lost to Nozomi Okuhara in a pulsating women’s singles final of the World Badminton Championships', 'India’s Olympic 2016 silver medallist PV Sindhu lost to Nozomi Okuhara in a pulsating women’s singles final of the World Badminton Championships', 'India’s Olympic 2016 silver medallist PV Sindhu lost to Nozomi Okuhara in a pulsating women’s singles final of the World Badminton Championships', 'The BWF World Championships (formerly known as IBF World Championships, also known as the World Badminton Championships) is a badminton tournament sanctioned by Badminton World Federation (BWF)', 'Below is the list of the most successful player(s) in each category (listed according to their last title):']\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "Label of the given query : PERS\n",
            "qc : PERS\n",
            "qc :  PERS\n",
            "{'PERSON': ['Nozomi Okuhara', 'Nozomi Okuhara', 'Saina Nehwal', 'Nozomi Okuhara', 'Nozomi Okuhara', 'Nozomi Okuhara'], 'ORG': ['WBC', 'the World Rankings', 'PV Sindhu', 'PV Sindhu', 'PV Sindhu', 'the World Badminton Championships', 'Badminton World Federation', 'BWF'], 'CARDINAL': ['21'], 'TIME': [], 'DATE': ['2017', '2017', '2017', '2017', 'Thursday 3 August 2017'], 'GPE': ['Japan', 'Japan', 'India', 'India', 'India', 'India'], 'PERCENT': [], 'LAW': [], 'ORDINAL': [], 'NORP': [], 'LOC': [], 'EVENT': ['BWF World Championships', 'World Badminton Championships', 'World Badminton Championships', 'World Badminton', 'the World Badminton Championships', 'the World Badminton Championships', 'the World Badminton Championships', 'The BWF World Championships', 'IBF World Championships'], 'FAC': [], 'QUANTITY': [], 'MONEY': []}\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "possible answers:\n",
            "['Nozomi Okuhara', 'Nozomi Okuhara', 'Saina Nehwal', 'Nozomi Okuhara', 'Nozomi Okuhara', 'Nozomi Okuhara']\n",
            "DATAFRAME WITH COUNT:\n",
            "0    Nozomi Okuhara\n",
            "1      Saina Nehwal\n",
            "Name: WORD, dtype: object\n",
            "{'PERSON': [], 'ORG': [\"Women's Singles World Badminton Championship 2017\"], 'CARDINAL': [], 'TIME': [], 'DATE': [], 'GPE': [], 'PERCENT': [], 'LAW': [], 'ORDINAL': [], 'NORP': [], 'LOC': [], 'EVENT': [], 'FAC': [], 'QUANTITY': [], 'MONEY': []}\n",
            "QUESTION TOKENIZING TO NEGLECT FEW ANSWERS\n",
            "[]\n",
            "FINAL ANSWER1:\n",
            "Nozomi Okuhara\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "\n",
            " QUESTION \n",
            "846\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "Telugu Sentence:\tఅంతరిక్షంలోకి వెళ్ళిన మొదటి భారతీయ వ్యోమగామి ఎవరు?\n",
            "\n",
            "English Sentence:\tWho was the first Indian astronaut to go into space?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "BING SEARCH\n",
            "Top 10 URLS:\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\n",
            "https://en.wikipedia.org/wiki/Rakesh_Sharma\n",
            "\n",
            "https://en.wikipedia.org/wiki/Kalpana_Chawla\n",
            "\n",
            "https://www.indiatimes.com/news/india/10-facts-about-rakesh-sharma-the-first-indian-to-go-to-space-on-his-67th-birthday-249391.html\n",
            "\n",
            "https://www.answers.com/Q/Who_was_the_first_Indian_to_go_into_space\n",
            "\n",
            "https://economictimes.indiatimes.com/news/international/world-news/first-pakistani-astronaut-will-go-into-space-in-2022-information-minister/articleshow/66364609.cms\n",
            "\n",
            "https://www.theguardian.com/world/2018/aug/15/india-conduct-manned-space-mission-2022-modi\n",
            "\n",
            "https://www.quora.com/Who-are-some-famous-Indian-astronauts\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "\n",
            "Top_similarity_sent :  ['Wing Commander Rakesh Sharma of the Indian Air Force was the first Indian to go into space', 'Chawla was the first Indian woman to fly in space', 'He was admitted to the National Defence Academy as an air force plebe in July 1966[4] and was commissioned into the Indian Air Force as a pilot in 1970,[5] thereafter becoming the first man from India to go into space', 'Kalpana Chawla (July 1, 1961 – February 1, 2003) was an American astronaut, engineer, and the first woman of Indian origin to go to space', 'The former air force pilot Rakesh Sharma became the first Indian to go to space, in 1984 as part of a Soviet mission', 'After the completion of STS-87 post-flight activities, Chawla was assigned to technical positions in the astronaut office to work on the space station', '[3] He was selected on 20 September 1982 to become a cosmonaut and go into space as part of a joint programme between the Indian Air Force and the Soviet Interkosmos space programme', 'He is the only Indian citizen to travel in space, although there have been other astronauts with an Indian background who were not Indian citizens', 'In 1984, Sharma became the first Indian citizen to enter space when he flew aboard the Soviet rocket Soyuz T-11 launched from Baikonur Cosmodrome in the Kazakh Soviet Socialist Republic on 3 April 1984', 'He remains to date the only Indian to have been conferred this honour']\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "Label of the given query : PERS\n",
            "qc : PERS\n",
            "qc :  PERS\n",
            "{'PERSON': ['Rakesh Sharma', 'Kalpana Chawla', 'Rakesh Sharma'], 'ORG': ['the Indian Air Force', 'the National Defence Academy', 'Indian Air Force', 'Chawla', 'the Indian Air Force', 'Soyuz'], 'CARDINAL': ['3'], 'TIME': [], 'DATE': ['July 1966[4', 'July 1, 1961', 'February 1, 2003', '1984', '20 September 1982', '1984', '3 April 1984'], 'GPE': ['India', 'Sharma', 'Baikonur', 'the Kazakh Soviet Socialist Republic'], 'PERCENT': [], 'LAW': [], 'ORDINAL': ['first', 'first', 'first', 'first', 'first', 'first'], 'NORP': ['Indian', 'Indian', 'American', 'Indian', 'Indian', 'Soviet', 'Soviet', 'Interkosmos', 'Indian', 'Indian', 'Indian', 'Indian', 'Soviet', 'Indian'], 'LOC': [], 'EVENT': [], 'FAC': [], 'QUANTITY': [], 'MONEY': []}\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "possible answers:\n",
            "['Rakesh Sharma', 'Kalpana Chawla', 'Rakesh Sharma']\n",
            "DATAFRAME WITH COUNT:\n",
            "0     Rakesh Sharma\n",
            "1    Kalpana Chawla\n",
            "Name: WORD, dtype: object\n",
            "{'PERSON': [], 'ORG': [], 'CARDINAL': [], 'TIME': [], 'DATE': [], 'GPE': [], 'PERCENT': [], 'LAW': [], 'ORDINAL': ['first'], 'NORP': ['Indian'], 'LOC': [], 'EVENT': [], 'FAC': [], 'QUANTITY': [], 'MONEY': []}\n",
            "QUESTION TOKENIZING TO NEGLECT FEW ANSWERS\n",
            "[]\n",
            "FINAL ANSWER1:\n",
            "Rakesh Sharma\n",
            "\n",
            "-----------------------------------------------------------\n",
            "\n",
            "\n",
            "ACTUAL ANS\n",
            "\n",
            "['6', '4', '2', '235', '5 సార్లు', '366 రోజులు', '4', '19', '4', '2', '4', '4', '20', '8', '206', '5', '3', '7', '4 సంవత్సరాలు', '36', 'రెండు సార్లు', '32', '4', '26 జనవరి', '11', '250', '3', '6', '11', '28 గ్రాములు', '8', '8', '0', '3వ స్థానము', '4', 'LIX', '144', '97', '21 అడుగులు', '8', '10000', '3', '7516 కిలో మీటర్లు', '24', '120', '12', '2.09 మీటర్లు', '640', '4', '1800 180 1947', 'రూ. 40', '4', '0.5', '4', '5', '10', '28.3 లీటర్లు', '485', '780', '127 కోట్లు', '8అడుగుల 11 అంగుళాలు', '14 అడుగులు', '91', '2', '6', '15', '2 శాతము', '7', '9.79', '8', '10000 లీటర్లు', '16', '43', '4', '7', '12', '6', '12', '42', '5050', '193', '962', '15', '1024', '6', '15000', '4840 చదరపు గజాలు', '16', '61', '2', '22 గజాలు', '3', '22', '4', '545', '5', '6 అంకెలు', '65536', '40', '19', '256', '7', '4 కి. కేలరీలు', '3 X భుజము', '4', '10', '6', '5', '7', '6', '7', '1800222021', '1', '4', '16', '5', '3.28', '4.18', '100', '118', '7వ', '18', '18', '7', '1000', 'ఒకసారి', '1', '5', '0 శీర్షాలు', '135 కోట్లు', '35 సంవత్సరాలు', '32', '0', 'మూడో', '24', '10', '90', '360', '8', '9', '100', '11', '99', '105', '750', '24', '57', 'రెండు', '2', '9899', '24 గంటలు', '8 నిమిషాల 16.6 సెకన్లు', '12 గంటల 26 నిమిషాలు', '1 సెకను తర్వాత', '24 గంటలు', '48 గంటలు', '9.63 సె.', '01', '5 గంటల 30 నిమిషాలు', '35 సంవత్సరములు', '72 నెలలు', '27 రోజులు', '16.30 గంటలు', '7.2 కి.మీ', '25 రోజులు', '45 సెకన్లు', '1977', '1948', '3, అక్టోబర్ 1990', '1757', '26 నవంబర్ 1949', '11 జూలై', '7 మే', '8 మే', '1928', '1964', '18 జూలై', 'ఫిబ్రవరి', 'ఫిబ్రవరి 23', 'ఆగష్టు  29', '2014', '14 సెప్టెంబర్', 'సెప్టెంబర్ 8', 'శనివారం', '21 జూన్', '5  సెప్టెంబర్', '15 జనవరి', '2 సంవత్సరాలు', 'ఆగస్ట్ 15', '29 రోజులు', 'డిసెంబర్ 25', 'గురువారం', 'డిసెంబర్  22', '1974', '1959', 'డిసెంబర్ 10', 'జనవరి 12', 'నవంబర్ 26, 1950', 'డిసెంబర్ 23', '120  రోజులు', 'అక్టోబర్ 18', '1780', 'అక్టోబరు 31', 'డిసెంబర్ 9', 'ఆగష్టు 9', '2014', '1956', 'ఆగష్టు 12', 'ఆగష్టు 21', '1990', '2013', 'మే 4', 'జూన్  12', 'మార్చ్  1', 'మార్చ్ 6', '15 ఆగష్టు 1995', 'డిసెంబర్ 10', 'ఫిబ్రవరి 28', 'ఏప్రిల్ 2,1984', 'నవంబర్ 14', '5 సంవత్సరములు', '1947 ఆగస్ట్ 15', 'డిసెంబర్ 10', 'అక్టోబర్ 2', '1881', 'జులై 11', 'మే  31', 'అక్టోబర్ 10', 'ఏప్రిల్ 18', '22 ఏప్రిల్', 'మార్చి 8', '1885', '1993', '365', 'డిసెంబర్', 'జూన్', 'అక్టోబర్ 5', '2005', '8 అక్టోబర్', 'నవంబర్ 14', '16 ఏప్రిల్1853', 'సైనిక  దినోత్సవం', '25 జనవరి', 'మే 31', '1972', '29 ఆగష్టు', '2 అక్టోబర్ 1869', '31 అక్టోబర్ 1984', '1962', 'అక్టోబర్', 'నవంబర్ 5, 2013', 'జనవరి 26', '14 జూన్', '8 వ సెప్టెంబర్', 'మార్చి 21', '1995', '1905', '30 జనవరి 1948', '8 ఆగస్టు 1942', '1862', 'మే 7', '14 నవంబర్', '1965', '2000', 'మే మొదటి ఆదివారం', '1999', '1947', '1939', '1963', '2005–2015', '20 ఆగస్టు', '5 సంవత్సరాలు', '21 సంవత్సరాలు', '25 సంవత్సరాలు', '35 సంవత్సరాలు', '6 సంవత్సరములు', 'ఒట్టావా', 'బంగ్లాదేశ్', 'మైసూర్', 'ఉత్తరాఖండ్', 'జర్మనీ', 'ఆంద్ర ప్రదేశ్', 'సూరీనామ్', 'చైనా', 'ఛత్తీస్ ఘఢ్', 'న్యూఢిల్లీ', 'లియోన్, ఫ్రాన్స్', 'బీజింగ్', 'ఒడిశా ', 'కెనడా', 'భారతదేశం', 'కాన్బెర్రా', 'అమెరికా', 'బ్రెజిల్', 'హైదరాబాద్', 'అగర్తల', 'గ్రేట్ బ్రిటన్', 'కేరళ', 'గుజరాత్', 'ఆస్ట్రేలియా', 'మధ్య ప్రదేశ్', 'న్యూయార్క్ నగరం', 'అమృత్సర్', 'సిలోన్', 'మాస్రం', 'ఇంగ్లాండ్', 'ఉత్తర ప్రదేశ్', 'ముంబై', 'కర్ణాటక', 'గార్డెన్ సిటీ', 'కర్నాటక', 'ఉత్తర ప్రదేశ్', 'గాందీనగర్', 'జైపూర్', 'జపాన్', 'కేరళ', 'ఆసియా', '28', 'రాంచీ', 'ముంబాయి', 'అస్సాం', 'చైనా', 'కోల్ కత్తా', 'బరియో డి జనీరో', 'బోధ గయ', 'అసోం', 'మసీదు', 'దక్షిణ కొరియా', 'సారనాథ్', 'రష్యా', 'అల్జీరియా', 'చైనా', 'స్వీడన్ ', 'చైనా', 'భారతదేశం ', 'ఆస్ట్రేలియా', 'కేరళ', 'జార్ఖండ్', 'పారిస్ ', 'పాటియాలా', 'ఢిల్లీ', 'దింఢి', 'కర్నాటక', 'అస్సాం', 'లోయ', 'కెనడా', 'న్యూ ఢిల్లీ', 'పూణె', 'పంజాబ్', 'పాకిస్తాన్', 'లండన్', 'ఉత్తర్ ప్రదేశ్ ', 'బ్రెజిల్', 'ముంబై', 'ఆస్ట్రేలియా', 'కేరళ', 'బీజింగ్', 'యార్లుంగ్ జాంగ్బో గ్రాండ్ కాన్యన్', 'హైదరాబాద్ ', 'హైదరాబాద్ ', 'టోక్యో ', 'హెమిస్ నేషనల్ పార్క్, జమ్మూ కాశ్మీర్ ', 'న్యూఢిల్లీ ', 'సెయింట్ లూయిస్', 'ఢాకా', 'పూనే', 'బ్రెజిల్', 'ఉత్తరప్రదేశ్', 'కోల్\\u200cకతా, పశ్చిమ బెంగాల్', 'మౌంట్ ఎవరెస్ట్', 'శ్రావణ బెళగొళ', 'మహబలిపురం', 'కొలన్ పాక', 'డిల్లీ', 'బీహార్', 'తిరుపతి', 'మహరాష్ట్ర', 'ఆసియా', 'రాజస్థాన్', 'గ్రీన్ లేండ్', 'మెక్సికో', 'కన్నౌజ్, ఉత్తర ప్రదేశ్', 'కొలంబో', 'ఔరంగాబాద్', 'ముంబాయి', 'ఆంధ్రప్రదేశ్', 'భారతదేశం', 'మావ్ సైన్రమ్', 'చైనా ', 'ఇటలి', 'ఖాట్మాండు', 'షాంఘై', 'ఒడిశా ', 'వాషింగ్టన్, డి.సి, అమెరికా. ', 'జపాన్ ', 'లండన్ ', 'లక్షద్వీప్ ', 'వెనిజులా', 'స్కాట్ ల్యాండ్', 'ఢిల్లీ', 'ఎర్రకోట', 'పెసిఫిక్ మహాసముద్రం', 'దక్షిణ కొరియా', 'ఇండోనేసియా', 'దక్షిణ ఆఫ్రికా', 'చైనా ', 'చెన్నై', 'కేరళ', 'లక్నో', 'బ్రిటన్', 'హైదరాబాద్', 'బోలోగ్నా విశ్వవిద్యాలయం ', 'యునైటెడ్ కింగ్డమ్', 'భారతదేశం', 'స్పెయిన్', 'గుజరాత్', 'దుబాయ్ ', 'చండీగఢ్', 'కోలకతా', 'నెదర్లాండ్', 'మాలే', 'పూంచ్', 'గుజారత్', 'గ్రేట్ బ్రిటన్', 'ఒసాకా', 'చైనా', 'న్యూఢిల్లీ', 'తెలంగాణ', 'మహారాష్ట్ర', 'మదన్ మోహన్ మల్వియా', 'తింపూ', 'ఉత్తర ప్రదేశ్', 'కోలకతా', 'మయన్మార్', 'పశ్చిమ బెంగాల్', 'జెనీవా', 'పుల్లెల గోపిచంద్', 'ఇరాక్', 'ఢిల్లీ', 'గుజరాత్', 'ఇండోనేషియా', 'శ్రీలంక', 'కోలకతా', 'కెనడా', 'రష్యా', 'స్పెయిన్', 'దక్షిణ అమెరికా', 'ఇండోనేషియా', 'మహారాష్ట్ర', 'గోరఖ్ పూర్ రైల్వే స్టేషన్', 'ఉత్తర అమెరికా', 'చండీగఢ్', 'బీహార్', 'టర్కీ', 'విశాఖపట్నం పోర్ట్', 'బంగ్లాదేశ్', 'ముంబై-అహ్మదాబాద్', 'అరుణాచల్ ప్రదేశ్', 'స్పెయిన్', 'ఇంగ్లాండ్', 'ఆఫ్గనిస్తాన్', 'నేపాల్', 'రష్యా', 'బ్రెజిల్', 'కర్ణాటక', 'రాజస్థాన్', 'తమిళనాడు', 'ఢిల్లీ', 'మధ్యప్రదేశ్', 'ఆస్ట్రేలియా', 'బ్రెజిల్', 'బ్రెజిల్', 'ఉత్తరాఖండ్', 'ఇంగ్లాండ్', 'దక్షిణ అమెరికా', 'ఆస్ట్రేలియా', 'రష్యా', 'బీహార్', 'రాజస్థాన్', 'అమితాబ్ బచ్చన్', 'ఆస్ట్రేలియా', 'ఇంగ్లాండ్', 'ఇంగ్లాండ్', 'ఆస్ట్రేలియా', 'అస్సాం', 'కాలిఫోర్నియా', 'ఒడిషా', 'బీహార్', 'పాట్నా', 'గుజరాత్', 'ఉత్తరాంచల్', 'దక్షిణ ఆఫ్రికా', 'జపాన్', 'ఆసియా', 'జార్ఖండ్', 'ఆస్ట్రేలియా', 'భారతదేశం', 'హిమాచల్ ప్రదేశ్', 'రాంచీ', 'గుజరాత్', 'ఆఫ్రికా', 'కాబూల్', 'పశ్చిమబెంగాల్', 'ఒడిషా', 'ఒడిషా', 'ఇంగ్లాండ్', 'పశ్చిమబెంగాల్', 'అలహాబాద్', 'చైనా', 'గుజరాత్', 'న్యూఢిల్లీ', 'పోర్ట్ బ్లెయిర్', 'ఆస్ట్రేలియా', 'ఉత్తరప్రదేశ్', 'రాజస్థాన్', 'మహారాష్ట్ర', 'ఉరుగ్వే', 'వాషింగ్టన్ డి.సి.', 'యు ఎస్ ఏ', 'యూరోప్', 'మాడ్రిడ్', 'జపాన్', 'యు ఎస్ ఏ', 'న్యూఢిల్లీ', 'భారతదేశం', 'కోలకతా', 'తమిళనాడు', 'పశ్చిమబెంగాల్', 'చత్తీస్గఢ్', 'బంగ్లాదేశ్', 'మధ్యప్రదేశ్', 'భారతదేశం', 'గుజరాత్', 'స్విట్జర్లాండ్', 'అర్జెంటీనా', 'రోమ్', 'మహారాష్ట్ర', 'తమిళనాడు', 'ఇటలీ', 'గుజరాత్', 'మధ్యప్రదేశ్', 'ఉత్తరప్రదేశ్', 'బీహార్', 'మధ్యప్రదేశ్', 'ఆఫ్రికా', 'దక్షిణ అమెరికా', 'వెల్లింగ్టన్', 'కోల్ కతా', 'ఫ్రాన్స్', 'ఈజిప్ట్', 'ఫ్రాన్స్', 'బీహార్', 'రాజస్థాన్', 'సుందర్ బాన్స్', 'డెన్మార్క్', 'అంటార్కిటికా', 'కన్యాకుమారి', 'ఆఫ్రికా', 'కజాఖస్తాన్', 'షిల్లాంగ్', 'తమిళనాడు', 'సిమ్లా', 'కెన్యా', 'సిక్కిం', 'గుజరాత్', 'గాంధీనగర్', 'ఉత్తర ప్రదేశ్', 'శ్రీలంక', 'ఉత్తరాంచల్', 'తెలంగాణ', 'బాంబే', 'దక్షిణాఫ్రికా', 'అండమాను మరియు నికోబార్ దీవులు', 'దక్షిణ కొరియా', 'న్యూఢిల్లీ', 'పాట్నా', 'హైదరాబాద్', 'హిమాచల్ ప్రదేశ్', 'ఆల్ఫ్రెడ్ నోబెల్', 'అక్బర్', 'అక్బర్', 'పవన్ కుమార్ చాంలిగ్', 'లెవిస్ కారోల్', 'థామస్ ఎడిసన్', 'క్రైస్తవులు', 'దిలీప్ కుమార్', 'లాల్ బహదూర్ శాస్త్రి', 'కాళిదాసు', 'సోమనాథ్ శర్మ', 'సుచేతా కృపలా', 'కరుణ్ నాయర్', 'జె జయలిలత', 'మిల్కా సింగ్', 'ప్రతిభా పాటిల్', 'క్రిస్టియన్ బెర్నార్డ్', 'సావిత్రి దేవి', 'కరుణ్ నాయర్', 'జవహర్ లాల్ నెహ్రూ', 'ఇబ్రహీం లోడి', 'పాస్కల్', 'కరణం మల్లీశ్వరి', 'చంద్రగుప్త మౌర్యడు', 'బ్రియాన్ లారా', 'రోహిత్ శర్మ', 'నీల్ ఆర్మ స్ట్రాంగ్', 'అక్షయ్ కుమార్', 'కె ఎల్ రాహుల్', 'వాస్కో డా గామా', 'ఖుతుబుద్దీన్   ఐబక్', 'రాజా రామ్ మోహన్ రాయ్', 'శివరాజ్ సింగ్ చౌహాన్', 'ఆరతి సాహా', 'బంకిం చంద్ర ఛటర్జీ', 'లియో టాల్స్టాయ్', 'జోనాథన్ స్విఫ్ట్', 'రిజర్వు బ్యాంక్ ఆఫ్', 'ఎపిజె అబ్దుల్ కలాం', 'విరాట్ కోహ్లీ', 'మహమ్మద్ అజారుద్దీన్', 'అబుల్ ఫజల్', 'సునీల్ చిత్రి', 'మార్టినా హింగిస్', 'జహీర్ ఖాన్', 'శశాంక్ మనోహర్', 'కుమార్ సంగక్కర', 'సర్దార్ పటేల్', 'సుభాష్ చంద్ర బోస్', 'షాజహన్', 'లాలా లజపతి రాయ్', 'నీల్ ఆర్మ్స్ట్రాంగ్', 'జవహర్ లాల్ నెహ్రూ', 'దేవికా రాణీ', 'లక్ష్మీకాంత్ ఫర్సేఖర్', 'లార్డు క్లివ్', 'రీతా ఫరియా', 'నరేంద్ర మోడీ', 'రవీంద్రనాథ్ ఠాగూర్', 'మహాత్మా గాంధీ', 'ఆర్ కె నారాయణ', 'వాస్కో డి గామా', 'లాలా లజపతి రాయ్', 'అభినవ్ బింద్రా', 'జాన్ నేపియర్', 'ప్రొఫెసర్ అమర్త్యసేన్', 'కైలాష్ సత్యార్ధి మరియు మలాలా యూసప్ ఝా ఇరువురు', 'అరుణ్ జైట్లీ', 'అటల్ బిహారీ వాజ్ పాయి', 'రోనాల్డ్ రాస్', 'మనోజ్ కుమార్', 'రాజామాన్ సింగ్', 'విఎస్ సంపత్', 'గురునానక్ దేవ్', 'తులసీదాస్', 'కపిల్ దేవ్', 'ఛరకుడు', 'ఆచార్య వినోభా భావే', 'రోజర్ బేకన్', 'సచిన్ రమేష్ టెండూల్కర్', 'రోహిత్ శర్మ', 'జి.వి.మావళంకర్', 'బులా చౌదరి', 'జార్జి ఈస్టమన్', 'జవహార్ లాల్ నెహ్రు', 'ఇండియా ఫౌండేషన్, వారణాసి', 'రాష్ట్రపతి ప్రణబ్ ముఖర్జీ', 'ఏనుగు లక్ష్మణ కవి', 'రాంమోహన్ రాయ్', 'సందీప్ తులసి యాదవ్', 'రఘురాం రాజన్', 'మిషెల్లి  కాకడె', 'మంచి పుస్తకం', 'పింగళి వెంకయ్య', 'కల్యాణ్ జ్యోతి సేన్ గుప్తా', 'విధ్యాబాలన్', 'భారతదేశం', 'స్వామి దయానంద సరస్వతి', 'వికాస్ గౌడ', 'ఆర్కిమెడిస్', 'డా. ప్రసన్న కుమార్ మోహంతి, ఐ ఎ ఎస్.,', 'భారతి కృష్ణ తీర్థాజీ మహారాజ్', 'రావూరి భరద్వాజ', 'హర్మన్ ప్రీత్ కౌర్', 'జైడ్రునాస్ సావికాస్', 'ఆచార్య వినోభాభావే', 'అల్తమాస్ కబీర్', 'ఎస్. జానకి', 'మిథాలి రాజ్, ఇండియా', 'దారాసింగ్', 'చరకుడు', 'పోర్చుగీసు వారు', 'అశోకుడు', 'సమర్థ రామదాస్', 'బరాక్ ఒబామా', 'ఇవాన్ పావ్ లోవ్', 'వెస్ట్ ఇండీస్', 'పోర్చుగీసువారు', 'సైనా నెహ్వాల్', 'విలియమ్ హానా మరియు జోసెఫ్', 'విరాట్ కోహ్లి', 'బంకించంద్ర చటోపాధ్యాయ', 'నేషనల్ బుక్ ట్రస్ట్, ఇండియా', 'సెరెనా విలియమ్స్', 'సునీతా విలియమ్స్', 'సరోజిని నాయుడు', 'డాక్టర్ జగదీష్ చంద్రబోస్', 'ఎమ్ ఒ పి అయ్యంగార్', 'ఇండియా', 'సారా', 'విలియమ్ హిగిన్ బోదమ్', 'ఆర్ కె నారాయణన్', 'విష్ణు శర్మ', 'జవహర్ లాల్ నెహ్రూ', 'ఎ పి జె అబ్దుల్ కలాం', 'ఛాయ కుల్ శ్రేష్ఠ', 'అబ్దుల్ కలాం', 'డా. డి. సుబ్బారావు', 'ఎడ్వర్డ్ జెన్నర్', 'ఐజాక్ ఎమ్ సింగర్', 'రవీంద్రనాథ్ టాగూర్', 'బాన్ కీ మూన్', 'బున్ సెన్', 'అశోక', 'పోరస్', 'రాజా రామ్మోహన్ రాయ్', 'కె.ఎం. కరియప్ప', 'సునీత విలియమ్స్', 'ఫెర్నాండ్ మాగ్ లిమ్', 'ఫ్రెడ్రిచ్ ఫ్రోబెల్', 'యూక్లిడ్', 'కౌటీల్యుడు', 'దేవికారాణి', 'బెస్సెమర్', 'కరణం మల్లీశ్వరి', 'విశ్వనాథ్ ప్రతాప్ సింగ్', 'రావణుడు', 'అనిల్ కుంబ్లే', 'తిరు ఒ పన్నీరు సెల్వ్ం', 'సిద్ధార్థ', 'సల్మాన్ రష్దీ', 'వోల్టా', 'రోజర్ ఫెదరర్', 'డోనాల్డ్ ట్రంప్', 'అఖిలేష్ యాదవ్', 'ఇడప్పడి కె పళనిస్వామి', 'రోజర్ ఫెదరర్', 'కె ఎల్  రాహుల్', 'వి కురియన్', 'రాజేష్ గోపీనాథ్', 'కనిష్కుడు', 'చంద్రగుప్త మౌర్య', 'అన్షు జమ్సెన్పా', 'బెంగుళూర్', 'యోగి ఆదిత్యనాథ్', 'కిరణ్ బేడి', 'బంకిమ్ చంద్ర చటోపాధ్యాయ', 'ఎస్. ఎస్ రాజమౌళి', 'జూల్స్ వెర్న్', 'సచిన్ టెండ్రూల్కర్', 'విరాట్ కోహిలి', 'జవహర్ లాల్ నెహ్రూ', 'రవీంద్రనాథ్ ఠాగోర్', 'డా. కె.రాధాకృష్ణన్', 'డా. ఎ. పి జె. అబ్దుల్ కలాం', 'డా. ఎ.పి.జి. అబ్దుల్ కలాం', 'పి.వి.సింధూ', 'స్వామి వివేకానంద', 'అరవిందో ఘోష్', 'రవింద్రనాథ్ ఠాగూర్', 'మహాత్మా గాంధీ', 'విజయ లక్ష్మీ పండిట్', 'అక్బర్', 'సి రాజగోపాలచారి', 'మేరీ క్యూరీ', 'భూపేష్  బేగెల్', 'విరాట్ కోహ్లీ', 'నోవాక్ జొకోవిక్', 'ఎడ్మండ్ హిల్లరీ', 'మదర్ థెరిస్సా', 'అక్బర్', 'పి టి ఉషా', 'టామ్ క్రూజ్', 'శామ్యూల్ కోల్ట్', 'అటల్ బిహారీ వాజ్పేయి', 'లార్డ్ విల్లియం బెంటింక్', 'పి వి సింధు', 'ఆర్ కె నారాయణ్', 'పాల్ హెర్మాన్ ముల్లర్', 'జి.వి.మావలాంకర్', 'రణబీర్ కపూర్', 'ఆచార్య వినోబా భావే', 'కాళిదాస్', 'లార్డ్ క్లైవ్', 'హెచ్ డి  కుమారస్వామి', 'జవహర్ లాల్ నెహ్రూ', 'డాక్టర్ బి. ఆర్. అంబేద్కర్', 'సల్మాన్ ఖాన్', 'రాఫెల్ నాదల్', 'దీపికా పడుకొనే', 'ఎం. ఎస్. సుబ్బళ్లక్ష్మి', 'హెన్రీ బెకర్వెల్', 'అజిత్ కుమార్ దవల్', 'బిప్లాబ్ కుమార్ దేవ్', 'గౌతమ బుద్ధుడు', 'అశోకుడు', 'స్టాన్ లీ', 'మూన్ జే-ఇన్', 'విక్రమ్ సేథ్', 'స్టీఫెన్ హాకింగ్', 'అనిల్ కుంబ్లే', 'నిర్మలా సీతారామన్', 'మెహబూబా మఫ్తి  సయీద్', 'లాల్ బహదూర్ శాస్త్రి', 'సునీల్ గవాస్కర్', 'రాజీవ్ గాంధీ', 'వేలెంటినా తెరేష్కోవా', 'ఇందిరా గాంధీ', 'ఎ ఓ హ్యూమ్', 'జాన్ డన్లప్', 'అనిల్ కుంబ్లే', 'ఇందిరా గాంధీ', 'ఇర్ఫాన్ పఠాన్', 'జవహర్ లాల్ నెహ్రూ', 'సుభాష్ చంద్రబోస్', 'ముఖేష్ అంబానీ', 'షాజహాన్', 'కృష్ణుడు', 'చంద్ర బహదూర్ దాంగి', 'అనుష్క శర్మ', 'రోహిత్ శర్మ', 'సర్ రిచర్డ్ బర్టన్', 'మైకేల్ ఫెరడే', 'జవహర్ లాల్ నెహ్రూ', 'మహాత్మా గాంధీ', 'ఎ ఓ హ్యూమ్', 'సునీల్ గవాస్కర్', 'నికోలస్ కోపర్నికస్', 'స్వామి వివేకానంద', 'నరేష్ గోయల్', 'పియూష్ గోయల్', 'లార్డ్ మౌంట్ బాటన్', 'రామ్ నాథ్ కోవింద్', 'బింబిసారుడు', 'గోపాల్ కృష్ణ గోఖలే', 'అబుల్ ఫజల్', 'ఎడ్వర్డ్ జెన్నర్', 'నోజోమి ఒకుహారా', 'రాకేష్ శర్మ']\n",
            "\n",
            "-----------------------------------------------------------\n",
            "\n",
            "\n",
            "LABELS\n",
            "\n",
            "['NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'TIME', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'DATE', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'QUANT', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'QUANT', 'NUMB', 'NUMB', 'NUMB', 'QUANT', 'NUMB', 'NUMB', 'NUMB', 'QUANT', 'NUMB', 'NUMB', 'NUMB', 'CUR', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'QUANT', 'NUMB', 'NUMB', 'NUMB', 'QUANT', 'QUANT', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'PERC', 'NUMB', 'NUMB', 'NUMB', 'QUANT', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'QUANT', 'NUMB', 'NUMB', 'NUMB', 'QUANT', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'QUANT', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'TIME', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'NUMB', 'TIME', 'TIME', 'TIME', 'TIME', 'TIME', 'TIME', 'TIME', 'TIME', 'TIME', 'TIME', 'TIME', 'TIME', 'TIME', 'QUANT', 'TIME', 'TIME', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'TIME', 'DATE', 'TIME', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'TIME', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'TIME', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'NUMB', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'DATE', 'TIME', 'DATE', 'TIME', 'TIME', 'TIME', 'TIME', 'TIME', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'NUMB', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'ORGA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'PERS', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'PERS', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'PERS', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'LOCA', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'ORGA', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'ORGA', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'LOCA', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'ORGA', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'LOCA', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'LOCA', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS', 'PERS']\n",
            "\n",
            "-----------------------------------------------------------\n",
            "\n",
            "\n",
            "FINAL ANS\n",
            "\n",
            "['Nozomi Okuhara', 'Rakesh Sharma']\n",
            "TOTAL TIME FOR QUESTIONS in sec :\n",
            "15.376704216003418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTthAx5VoZnz"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oRj9LzBoZn5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}